<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Hongbo Sun (Â≠ôÂÆèÂçö) - Homepage</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        margin: 0;
        background: #f9f9f9;
        color: #333;
        line-height: 1.6;
      }

      nav {
        background: #800000;
        display: flex;
        justify-content: center;
        gap: 20px;
        padding: 10px 0;
        position: sticky;
        top: 0;
        z-index: 100;
      }

      nav a {
        color: white;
        text-decoration: none;
        font-weight: bold;
      }

      nav a:hover {
        text-decoration: underline;
      }

      .container {
        margin: 20px 150px 20px 100px;
        display: flex;
      }

      /* Â∑¶ËæπÂõ∫ÂÆöÂ§¥ÂÉèÊ†è */
      .sidebar {
        width: 220px;
        flex-shrink: 0;
        position: sticky;
        top: 80px;
        align-self: flex-start;
        text-align: center;
        padding-right: 20px;
      }

      .sidebar img {
        width: 160px;
        border-radius: 10px;
        margin-bottom: 10px;
      }

      .subtitle {
        font-size: 14px;
        color: #666;
        margin-bottom: 10px;
      }

      .contact a {
        display: block;
        color: #800000;
        text-decoration: none;
        font-size: 14px;
        margin: 3px 0;
      }

      .contact a:hover {
        text-decoration: underline;
      }

      /* Âè≥Ëæπ‰∏ªË¶ÅÂÜÖÂÆπ */
      .main {
        flex: 1;
      }

      .main section {
        margin-bottom: 40px;
      }

      h2 {
        border-bottom: 2px solid #ddd;
        padding-bottom: 5px;
        margin-top: 10px;
      }

      ul {
        padding-left: 20px;
      }

      footer {
        background: #800000;
        color: white;
        text-align: center;
        padding: 15px;
        margin-top: 40px;
      }

      .paper {
        display: flex;
        align-items: center;
        gap: 20px;
        margin-bottom: 30px;
        background: #fff;
        padding: 15px;
        border-radius: 10px;
        box-shadow: 0 2px 6px rgba(0, 0, 0, 0.1);
      }

      .paper img {
        width: 200px;
        height: 125px;
        object-fit: fill;
        border-radius: 5px;
      }

      .paper h3 {
        margin: 0;
        font-size: 18px;
        color: rgb(55, 84, 141);
        font-family: "Times New Roman", Times, serif;
      }

      .paper p {
        margin: 5px 0;
      }

      .work h3 {
        margin: 0 0 0 15px;
        font-size: 18px;
      }

      .work p {
        margin: 0 0 20px 15px;
      }

      #news a {
        text-decoration: none;
        /* ÈªòËÆ§Êó†‰∏ãÂàíÁ∫ø */
      }

      #news a:hover {
        text-decoration: underline;
      }
    </style>

    <link rel="stylesheet" type="text/css" href="stylesheet.css">

  </head>

  <body>
    <nav>
      <a href="#about">About</a>
      <a href="#news">News</a>
      <a href="#publications">Publications</a>
      <!-- <a href="#related-work">Related Work</a> -->
      <a href="#education">Education</a>
      <a href="#honors">Honors</a>
      <a href="#activities">Activities</a>
    </nav>

    <div class="container">
      <!-- Â∑¶ËæπÂ§¥ÂÉèÊ†è -->
      <aside class="sidebar">
        <img src="images/sunhongbo.png" alt="Profile Photo" />
        <div class="subtitle">
          Research Interests: Multimodal Large Language Model (MLLM), Multimodal Content
          Understanding, Fine-Grained Visual Analysis
        </div>
        <div class="contact">
          Researcher at TeleAI <br />(Institute of Artificial Intelligence (TeleAI), China Telecom)<br />

          <a href="https://orcid.org/0000-0002-2639-9035">ORCID</a>
          <a
            href="https://scholar.google.com.hk/citations?user=w48SDn8AAAAJ&hl=zh-CN&oi=ao"
            >Google Scholar</a
          >
          <a href="mailto:sunhongbo@pku.edu.cn">sunhongbo@pku.edu.cn</a>
        </div>
      </aside>

      <!-- Âè≥Ëæπ‰∏ªË¶ÅÂÜÖÂÆπ -->
      <main class="main">
        <section id="about">
          <h2>üë®‚Äçüíª Hongbo Sun (Â≠ôÂÆèÂçö) <a href="https://sunhongbo1995.github.io/">[‰∏≠ÊñáÁâà]</a></h2>
          <p style="text-align: justify; text-justify: inter-ideograph">
       I am currently an AGI researcher at the Institute of Artificial Intelligence (TeleAI), China Telecom, where I focus on developing Multimodal Large Language Models (MLLMs) and advancing their applications in downstream domains. As a core contributor, I participated in the development of China Telecom‚Äôs multimodal large language model TeleMM and the TeleSearch 2.0 system designed for ubiquitous surveillance. TeleMM ranked first,
            second, and third, respectively, on the international authoritative
            benchmarks MMMU, MME, and the domestic authoritative benchmark 
            <a
              href="https://rank.opencompass.org.cn/leaderboard-multimodal/?m=24-12"
              style="text-decoration: none"
              ><b style="color: rgb(51, 122, 183)"
                >OpenCompass (2024 overall
                leaderboard)</b
              ></a
            >. 

            As a core R&D engineer, I led the research and development of the multimodal reasoning large model TeleMM-2.0-Thinking, which ranked 2nd in the domestic authoritative benchmark <a href="https://rank.opencompass.org.cn/leaderboard-multimodal/?m=25-12" style="text-decoration: none;"><b style="color: rgb(51, 122, 183);">OpenCompass (2025 Overall Leaderboard)</b></a>. The model demonstrates strong capabilities in visual reasoning and hallucination mitigation, while achieving industry-leading performance in interdisciplinary reasoning, mathematical computation, table/chart analysis, and spatial reasoning. Currently deployed across multiple application scenarios including government affairs, manufacturing, transportation, and public security, it delivers efficient and reliable multimodal intelligent solutions.


          </p>
          <p style="text-align: justify; text-justify: inter-ideograph">
            
            I obtained my Ph.D. degree in Computer Applied Technology at Peking
            University (PKU) in 2024, earning the Excellent Doctoral Dissertation
            Award of Beijing Society of Image and Graphics (BSIG).  I was selected into Young Elite Scientists Sponsorship Program of the Beijing High Innovation Plan in 2025. My research interests include MLLM, multimodal content understanding, and fine-grained visual analysis.
            
          </p>

        <p style="text-align: justify;"> <font color="red"><strong>I am actively open to academic collaborations and recruiting Research Interns at TeleAI. Welcome to contact me with your detailed CV!ÔºàEmail: sunhb3@chinatelecom.cnÔºâ</strong></font>
		    </p>




        </section>

        <section id="news">
          <h2>üî• News</h2>
          <ul>


          <li>
            <b>2025.12</b> ‚Äì <a href="https://mp.weixin.qq.com/s/ArGBwQvaP0m6x0eaRM6IBQ?xtrack=1&scene=90&subscene=93&sessionid=1768046612&flutter_pos=0&clicktime=1768046614&enterid=1768046614&finder_biz_enter_id=4&jumppath=50094_1768045147185,1101_1768045156449,1101_1768045164972,50094_1768046612976&jumppathdepth=4&ascene=56&realreporttime=1768046614240&forceh5=1&devicetype=android-36&version=4.1.39.70527&nettype=WIFI&abtest_cookie=AAACAA==&lang=zh_CN&session_us=gh_799d2052d1c6&countrycode=CN&exportkey=n_ChQIAhIQfXFzy+mSVh1RWZU5zR9g4BLxAQIE97dBBAEAAAAAAOk2LcJS7ywAAAAOpnltbLcz9gKNyK89dVj010Iz2QIVf6ytR6FWm31vhvri2xnlHzFcdBUASY7GXbqvwVWLK6ROb7aToOBgYUHTVLShi/HkQEuwUgUiPkRnHY/w31bgkRt8xmszU488VzSynQHzVkNY3qdo/zEizo3zytMLHhBm6b3TDnP1/Bb8gmbZPSSKERmoAXFYA6bg17XtmXWHVCUu/sdXx+g2Fq3LXhrVIpzbsLd8jnR0BW6i57YUO44kS1miw6LkmFQin4WgTHCzDoGhG6i2wCvISKeJKL3tooBtF6maH24=&pass_ticket=T3uhEQFKlmOtGCQIPb7PxXLsrBYLR2Ec+DhTIyacRDIv6gMk7Hknk5/21h+VmMFi&wx_header=3&platform=mac"><b style="color: rgb(51, 122, 183);">&nbsp;TeleMM-2.0-Thinking ranked second and won the silver medal on the OpenCompass Multi-modal Academic Leaderboard.&nbsp;</b></a>
          </li>


          <li>
            <b>2025.12</b> ‚Äì Release an open-source referring expression comprehension benchmark project for multimodal large language models (MLLMs)<a href="https://jerrypw.github.io/RefBench-PRO.project_page/"><b style="color: rgb(51, 122, 183);">&nbsp;RefBench-PRO&nbsp;</b></a>„ÄÇ
          </li>




            <li><b>2025.11</b> ‚Äì 1 paper accepted by AAAI 2026.</li>

            <li>
              <b>2025.10</b> ‚Äì Ranked 3rd in the ICCV 2025 Multimodal Large Language
              Model Visual Reasoning Localization Challenge<a
                href="https://arxiv.org/pdf/2509.14142"
                ><b style="color: rgb(51, 122, 183)"
                  >&nbsp;MARS2: VG-RS&nbsp;</b
                ></a
              >.
            </li>

            <li>
              <b>2025.09</b> ‚Äì Released the open-source reinforcement learning project for video
              understanding large model <a
                href="https://github.com/Hui-design/TSPO"
                ><b style="color: rgb(51, 122, 183)">&nbsp;TSPO&nbsp;</b></a
              >, ranking top five in VideoMME, LVBench, and MLVU.
            </li>

            <li>
              <b>2025.07</b> ‚Äì Selected into Young Elite Scientists Sponsorship Program of the Beijing High Innovation Plan, 2025.
            </li>

            <li>
              <b>2024.12</b> ‚Äì Developed TeleMM as the core contributor, which ranked 3rd on
              the OpenCompass 2024 overall leaderboard, surpassing GPT-4o available at that time. It has been widely applied as the foundational model in China Telecom's businesses such as social security, urban governance,
              and traffic management.
            </li>

            <li><b>2024.09</b> ‚Äì 1 paper accepted by TIP 2024.</li>

            <li>
              <b>2024.08</b> ‚Äì Excellent Doctoral Dissertation Award of Beijing Society of Image and Graphics (BSIG), 2024
            </li>

            <li>
              <b>2024.07</b> ‚Äì Joined Institute of Artificial Intelligence (TeleAI), China Telecom, as a AGI researcher, responsible for developing Multimodal Large Language Models and advancing their applications in downstream domains.
            </li>

            <li>
              <b>2024.07</b> ‚Äì Attained a Ph.D. degree in Computer Application Technology at Peking University (PKU).
            </li>
          </ul>
        </section>

        <section id="publications">
          <h2>üìù Selected Publications</h2>


          <div class="paper">
            <img src="images/RefBench_PRO.png" alt="RefBench-PRO" />
            <div>
              <h3>
                RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension
              </h3>
              <p>
                Tianyi Gao, Hao Li, Han Fang, Xin Wei, Xiaodong Dong, <b>Hongbo Sun</b>, Ye Yuan, Zhongjiang He, Jinglin Xu, Jingmin Xin, Hao Sun<br />
                <i>arXiv preprint arXiv:2512.06276, 2025.</i>
                [<a href="https://arxiv.org/pdf/2512.06276" target="_blank"><b>Paper</b></a>]&nbsp;[<a
                  href="https://jerrypw.github.io/RefBench-PRO.project_page/" target="_blank"><b>Project</b></a>]
                </p>
            </div>
          </div>


          <div class="paper">
            <img src="images/tspo.png" alt="TSPO" />
            <div>
              <h3>
                TSPO: Temporal Sampling Policy Optimization for Long-form Video
                Language Understanding
              </h3>

              <p>
              Canhui Tang, Zifan Han, <b>Hongbo Sun</b>, Sanping Zhou, Xuchong Zhang,
              Xin Wei, Ye Yuan, Huayu Zhang, Jinglin Xu and Hao Sun<br />
              <i>AAAI Conference on Artificial Intelligence (AAAI), 2026. (CCF A) (Accepted)</i>
              [<a href="https://arxiv.org/pdf/2508.04369" target="_blank"><b>Paper</b></a>]&nbsp;[<a
                href="https://github.com/Hui-design/TSPO?tab=readme-ov-file" target="_blank"><b>Code</b></a>]&nbsp;[<a
                  href="https://mp.weixin.qq.com/s/2k-P4_A26UtG-uXbH6tkMw"
                  target="_blank"
                  ><b>Reported by TeleAI</b></a
                >]
            
               </p>

            </div>
          </div>

          <div class="paper">
            <img src="images/simofe.png" alt="SIM-OFE" />
            <div>
              <h3>
                SIM-OFE: Structure Information Mining and Object-aware Feature
                Enhancement for Fine-Grained Visual Categorization
              </h3>
              <p>
                <b>Hongbo Sun</b>, Xiangteng He, Jinglin Xu and Yuxin Peng<br />
                <i
                  >IEEE Transactions on Image Processing (TIP), Vol. 33, pp.
                  5312‚Äì5326, 2024. (CCF A)</i
                >
                [<a
                  href="https://ieeexplore.ieee.org/abstract/document/10684043"
                  target="_blank"
                  ><b>Paper</b></a
                >]
              </p>
            </div>
          </div>

          <div class="paper">
            <img src="images/finefmpl.png" alt="FineFMPL" />
            <div>
              <h3>
                FineFMPL: Fine-grained Feature Mining Prompt Learning for
                Few-Shot Class Incremental Learning
              </h3>
              <p>
                <b>Hongbo Sun</b>, Jiahuan Zhou, Xiangteng He, Jinglin Xu and
                Yuxin Peng<br />
                <i
                  >Proceedings of the 33rd International Joint Conference on
                  Artificial Intelligence (IJCAI), Jeju, South Korea, Aug. 3-9,
                  2024. (CCF A)</i
                >[<a
                  href="https://www.ijcai.org/proceedings/2024/0144.pdf"
                  target="_blank"
                  ><b>Paper</b></a
                >]&nbsp;[<a
                  href="https://github.com/PKU-ICST-MIPL/FineFMPL_IJCAI2024"
                  target="_blank"
                  ><b>Code</b></a
                >]
              </p>
            </div>
          </div>

          <div class="paper">
            <img
              src="images/dualmodal.png"
              alt="Dual-Modal Adaptive Online Prompting"
            />
            <div>
              <h3>
                Dual-Modal Adaptive Online Prompting and Knowledge Retention for
                Test-Time Adaptation
              </h3>
              <p>
                Zichen Liu, <b>Hongbo Sun</b>, Yuxin Peng and Jiahuan Zhou<br />
                <i
                  >Proceedings of the 38th AAAI Conference on Artificial
                  Intelligence (AAAI), Vancouver, Canada, Feb. 20-27, 2024. (CCF
                  A)</i
                >
                [<a
                  href="https://ojs.aaai.org/index.php/AAAI/article/view/29320"
                  target="_blank"
                  ><b>Paper</b></a
                >]
              </p>
            </div>
          </div>

          <div class="paper">
            <img src="images/hcl.png" alt="HCL" />
            <div>
              <h3>
                HCL: Hierarchical Consistency Learning for Webly Supervised
                Fine-Grained Recognition
              </h3>
              <p>
                <b>Hongbo Sun</b>, Xiangteng He and Yuxin Peng<br />
                <i
                  >IEEE Transactions on Multimedia (TMM), Vol. 26, pp.
                  5108‚Äì5119, 2024.</i
                >
                [<a
                  href="https://hexiangteng.github.io/papers/TMM%202023.pdf"
                  target="_blank"
                  ><b>Paper</b></a
                >]&nbsp;[<a
                  href="https://github.com/PKU-ICST-MIPL/HCL_TMM2023"
                  target="_blank"
                  ><b>Code</b></a
                >]&nbsp;[<a
                  href="https://mp.weixin.qq.com/s/-BGHEXejE_1DjovYW7XagQ"
                  target="_blank"
                  ><b>Reported by CCF-MM</b></a
                >]
              </p>
            </div>
          </div>

          <div class="paper">
            <img
              src="images/vlprompt.png"
              alt="Fine-Grained Visual Prompt Learning"
            />
            <div>
              <h3>
                Fine-Grained Visual Prompt Learning of Vision-Language Models
                for Image Recognition
              </h3>
              <p>
                <b>Hongbo Sun</b>, Xiangteng He, Jiahuan Zhou and Yuxin Peng<br />
                <i
                  >Proceedings of the 31st ACM International Conference on
                  Multimedia (ACM MM), Ottawa, Canada, Oct. 29-Nov. 3, 2023.
                  (CCF A)</i
                >
                [<a
                  href="https://zhoujiahuan1991.github.io/pub/MM2023_Finegrained.pdf"
                  target="_blank"
                  ><b>Paper</b></a
                >]
              </p>
            </div>
          </div>

          <div class="paper">
            <img src="images/simtrans.png" alt="SIM-Trans" />
            <div>
              <h3>
                SIM-Trans: Structure Information Modeling Transformer for
                Fine-grained Visual Categorization
              </h3>
              
              <p>
              <b>Hongbo Sun</b>, Xiangteng He and Yuxin Peng<br />
              <i>Proceedings of the 30th ACM International Conference on
                Multimedia (ACM MM), Lisbon, Portugal, Oct. 10-14, 2022. (CCF
                A)(<font color="red"><strong>Oral, 5.9%</strong></font>)</i>
              [<a href="https://arxiv.org/pdf/2208.14607" target="_blank"><b>Paper</b></a>][<a
                href="https://github.com/PKU-ICST-MIPL/SIM-Trans_ACMMM2022" target="_blank"><b>Code</b></a>]
            </p>

            </div>
          </div>
        </section>

        <section id="education">
          <h2>üìñ Education</h2>
          <ul>
            <li>
              2019.09-2024.07 &emsp;Peking University &emsp; Computer Applied
              Technology &emsp; &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;Ph.D.
            </li>
            <li>
              2016.09-2019.01 &emsp;Tianjin University &emsp; Information and
              Communication Engineering &emsp;Master
            </li>
            <li>
              2012.09-2016.07 &emsp;Tianjin University &emsp; Electronic
              Information Engineering &emsp;
              &emsp;&emsp;&emsp;&emsp;&nbsp;Bachelor
            </li>
          </ul>
        </section>

        <section id="honors">
          <h2>üéñ Honors and Awards</h2>
          <ul>
            <li>
                          
              Selected into Young Elite Scientists Sponsorship Program of the Beijing High Innovation Plan, 2025

            </li>
            <li>
              Excellent Doctoral Dissertation Award of Beijing Society of Image and Graphics (BSIG), 2024
            </li>
            <li>
              First place in the TRECVID Video Instance Search Competition in 2019 and
              2020
            </li>
            <li>
              Third Prize in the National Finals of BOE Innovation Challenge, Achieved Special Offer from BOE Innovation Lab, 2016
            </li>
          </ul>
        </section>

        <section id="activities">
          <h2>üìù Academic Services</h2>
          <ul>
            <li>AAAI Program Committee Member</li>
            <li>
              Reviewer for top international conferences and journals (CVPR,
              ICCV, AAAI, IEEE TMM, etc.)
            </li>
          </ul>
        </section>
      </main>
    </div>

    <footer>
      ¬© 2025 Hongbo Sun, Institute of Artificial Intelligence (TeleAI), China Telecom. All rights reserved.
    </footer>
  </body>
</html>
